{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Input user credentials\n",
    "email_id = input(\"Enter your LinkedIn email: \")\n",
    "password_input = input(\"Enter your LinkedIn password: \")\n",
    "designation = input(\"Enter the job designation: \")\n",
    "location = input(\"Enter the job location: \")\n",
    "num_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "print(\"\\033[1mOpening LinkedIn and initiating login...\\033[0m\")\n",
    "\n",
    "# Sign in to LinkedIn\n",
    "sign_in = WebDriverWait(driver, 20).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"/html/body/main/section[1]/div/div/a\"))\n",
    ")\n",
    "sign_in.click()\n",
    "\n",
    "email_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_key\")))\n",
    "password_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_password\")))\n",
    "\n",
    "# Clear and send keys to the input fields\n",
    "email_element.clear()\n",
    "email_element.send_keys(email_id)\n",
    "password_element.clear()\n",
    "password_element.send_keys(password_input)\n",
    "\n",
    "login = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, \"btn__primary--large\")))\n",
    "login.click()\n",
    "\n",
    "print(\"\\033[1mLogin successful!\\033[0m\")\n",
    "\n",
    "# Navigate to Jobs section\n",
    "time.sleep(5)\n",
    "jobs_link = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[6]/header/div/nav/ul/li[3]/a\")))\n",
    "jobs_link.click()\n",
    "\n",
    "print(\"\\033[1mNavigated to Jobs section.\\033[0m\")\n",
    "\n",
    "# Construct the job search URL\n",
    "base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "search_url = f\"{base_url}?keywords={designation.replace(' ', '%20')}&location={location.replace(' ', '%20')}\"\n",
    "driver.get(search_url)\n",
    "\n",
    "print(f\"\\033[1mSearching for '{designation}' jobs in '{location}'...\\033[0m\")\n",
    "\n",
    "# List to store job links\n",
    "job_desc_links = []\n",
    "\n",
    "for page in range(num_pages):\n",
    "    print(f\"\\033[1mScraping Page {page + 1}...\\033[0m\")\n",
    "    \n",
    "    # Scrape job links from the current page\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, \"scaffold-layout__list-container\")))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        list_items = soup.find_all('li', class_=\"jobs-search-results__list-item\")\n",
    "\n",
    "        for item in list_items:\n",
    "            link = item.find('a', href=True)\n",
    "            if link:\n",
    "                job_desc_links.append(\"https://www.linkedin.com\" + link['href'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding job links on page {page + 1}: {e}\")\n",
    "        break\n",
    "\n",
    "    # Click on the pagination button for the next page\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f\"//button[@aria-label='Page {page + 2}']\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Small delay to allow the page to load\n",
    "    except (NoSuchElementException, TimeoutException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error navigating to the next page: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\033[1mTotal job links extracted: {len(job_desc_links)}\\033[0m\")\n",
    "\n",
    "# Scraping job details\n",
    "job_data = []\n",
    "\n",
    "for link in job_desc_links:\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract job title\n",
    "        div_element = soup.find('div', class_=\"t-24 job-details-jobs-unified-top-card__job-title\")\n",
    "        job_title = div_element.get_text(strip=True) if div_element else \"Title not provided\"\n",
    "\n",
    "        # Extract company name\n",
    "        company_div = soup.find('div', class_='job-details-jobs-unified-top-card__company-name')\n",
    "        company_name = company_div.get_text(strip=True) if company_div else \"Company not provided\"\n",
    "\n",
    "        # Extract location and date of posting\n",
    "        loc_date_div = soup.find('div', class_='job-details-jobs-unified-top-card__primary-description-container')\n",
    "        loc_date_span = loc_date_div.find_all('span', class_='tvm__text tvm__text--low-emphasis') if loc_date_div else []\n",
    "\n",
    "        job_location = \" \".join([span.get_text(strip=True).replace(\"\\u00b7\", \"\") for span in loc_date_span[:2]]) or \"Location not provided\"\n",
    "        post_date = \" \".join([span.get_text(strip=True).replace(\"\\u00b7\", \"\") for span in loc_date_span[2:]]) or \"Date not provided\"\n",
    "\n",
    "        # Extract work type\n",
    "        work_type_button = soup.find('button', class_='job-details-preferences-and-skills')\n",
    "        work_types = work_type_button.get_text(strip=True).replace(\"\\u20b9\", \"INR\") if work_type_button else \"Work types not provided\"\n",
    "\n",
    "        job_details = {\n",
    "            \"Job Title\": job_title,\n",
    "            \"Company\": company_name,\n",
    "            \"Portal Link\": \"https://www.linkedin.com/\",\n",
    "            \"Job Listing Link\": link,\n",
    "            \"Location\": job_location,\n",
    "            \"Date of Posting\": post_date,\n",
    "            \"Work Type\": work_types,\n",
    "        }\n",
    "        \n",
    "        job_data.append(job_details)\n",
    "        print(f\"\\033[1mJob details extracted for: {job_title} at {company_name}\\033[0m\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping job details from {link}: {e}\")\n",
    "\n",
    "# Save JSON data\n",
    "output_dir = os.path.join(os.getcwd(), f\"{designation}_{location}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for job in job_data:\n",
    "    try:\n",
    "        file_name = f\"{job['Company'].replace(' ', '_').replace('/', '_')}_{job['Location'].replace(' ', '_')}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(job, file, indent=4)\n",
    "        print(f\"\\033[1mJob details saved for: {job['Job Title']} at {job['Company']}\\033[0m\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving job data: {e}\")\n",
    "\n",
    "print(\"\\033[1mAll job details saved successfully!\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c33d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
