{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f07da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your LinkedIn email: jagadeeshjagan2003@gmail.com\n",
      "Enter your LinkedIn password: jagan@985\n",
      "Enter the job designation: Software Engineer\n",
      "Enter the job location: Hyderabad\n",
      "Enter the number of pages to scrape: 2\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",  # Save logs in a file\n",
    "    level=logging.INFO,  # Log INFO and above (INFO, WARNING, ERROR, CRITICAL)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    ")\n",
    "\n",
    "logging.info(\"Script started.\")\n",
    "\n",
    "# Input user credentials\n",
    "email_id = input(\"Enter your LinkedIn email: \")\n",
    "password_input = input(\"Enter your LinkedIn password: \")\n",
    "designation = input(\"Enter the job designation: \")\n",
    "location = input(\"Enter the job location: \")\n",
    "num_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.linkedin.com/login\")   \n",
    "\n",
    "    email_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_key\")))\n",
    "    password_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_password\")))\n",
    "\n",
    "    # Clear and send keys to the input fields\n",
    "    email_element.clear()\n",
    "    email_element.send_keys(email_id)\n",
    "    password_element.clear()\n",
    "    password_element.send_keys(password_input)\n",
    "\n",
    "    login = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, \"btn__primary--large\")))\n",
    "    login.click()\n",
    "    logging.info(\"Login successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Login failed: {str(e)}\")\n",
    "\n",
    "    \n",
    "# Navigate to Jobs section\n",
    "try:\n",
    "    jobs_link = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/div[6]/header/div/nav/ul/li[3]/a\"))\n",
    "    )\n",
    "    jobs_link.click()\n",
    "    logging.info(\"Navigated to Jobs section.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to navigate to Jobs section: {str(e)}\")\n",
    "# try:\n",
    "# #     desig_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//input[contains(@aria-label, 'Search job titles or companies')]\")))\n",
    "# #     location_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//input[contains(@aria-label, 'City, state, or zip code')]\")))\n",
    "    \n",
    "# #     desig_element.clear()\n",
    "# #     desig_element.send_keys(designation)\n",
    "# #     time.sleep(2)\n",
    "# #     location_element.clear()\n",
    "# #     location_element.send_keys(location)\n",
    "# #     time.sleep(2)\n",
    "# #     all_inputs = driver.find_elements(By.TAG_NAME, \"input\")\n",
    "#     # Get all input elements on the page and print their attributes\n",
    "# #     all_inputs = driver.find_elements(By.TAG_NAME, \"input\")\n",
    "# #     for index, input_field in enumerate(all_inputs):\n",
    "# #         print(f\"Input {index+1}:\")\n",
    "# #         print(f\"  Name: {input_field.get_attribute('name')}\")\n",
    "# #         print(f\"  Placeholder: {input_field.get_attribute('placeholder')}\")\n",
    "# #         print(f\"  Aria-label: {input_field.get_attribute('aria-label')}\")\n",
    "# #         print(\"-\" * 30)\n",
    "# # logging.info(f\"Searching for '{designation}' jobs in '{location}'...\")\n",
    "# # Switch to the iframe\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#         EC.frame_to_be_available_and_switch_to_it((By.ID, \"destination_publishing_iframe_lnkd_0\"))\n",
    "#     )\n",
    "\n",
    "#     # Locate job designation field\n",
    "#     designation_field = WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, \"//input[@placeholder='Search job titles or companies']\"))\n",
    "#     )\n",
    "#     designation_field.send_keys(\"Software Engineer\")\n",
    "\n",
    "#     # Locate location field\n",
    "#     location_field = WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, \"//input[@placeholder='City, state, or zip code']\"))\n",
    "#     )\n",
    "#     location_field.send_keys(\"Noida\")\n",
    "#     iframe_elements = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "#     for index, iframe in enumerate(iframe_elements):\n",
    "#         print(f\"Nested Iframe {index+1}: {iframe.get_attribute('name')} - {iframe.get_attribute('id')}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to load job search page: {str(e)}\")\n",
    "try:\n",
    "    base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "    search_url = f\"{base_url}?keywords={designation.replace(' ', '%20')}&location={location.replace(' ', '%20')}\"\n",
    "    driver.get(search_url)\n",
    "    logging.info(f\"Searching for '{designation}' jobs in '{location}'...\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load job search page: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ac79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your LinkedIn email: jagadeeshjagan2003@gmail.com\n",
      "Enter your LinkedIn password: jagan@985\n",
      "Enter the job designation: Software Engineer\n",
      "Enter the job location: Hyderabad\n",
      "Enter the number of pages to scrape: 2\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"linkedin_scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# User Inputs\n",
    "email_id = input(\"Enter your LinkedIn email: \")\n",
    "password_input = input(\"Enter your LinkedIn password: \")\n",
    "designation = input(\"Enter the job designation: \")\n",
    "location = input(\"Enter the job location: \")\n",
    "num_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/\")\n",
    "driver.maximize_window()\n",
    "logging.info(\"Opened LinkedIn homepage.\")\n",
    "\n",
    "# Login Process\n",
    "try:\n",
    "    sign_in = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/main/section[1]/div/div/a\"))\n",
    "    )\n",
    "    sign_in.click()\n",
    "    logging.info(\"Navigated to LinkedIn login page.\")\n",
    "\n",
    "    email_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_key\")))\n",
    "    password_element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, \"session_password\")))\n",
    "    \n",
    "    email_element.clear()\n",
    "    email_element.send_keys(email_id)\n",
    "    password_element.clear()\n",
    "    password_element.send_keys(password_input)\n",
    "\n",
    "    login = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CLASS_NAME, \"btn__primary--large\")))\n",
    "    login.click()\n",
    "    logging.info(\"Successfully logged into LinkedIn.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Login failed: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Navigate to Jobs Section\n",
    "try:\n",
    "    time.sleep(5)\n",
    "    jobs_link = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[6]/header/div/nav/ul/li[3]/a\")))\n",
    "    jobs_link.click()\n",
    "    logging.info(\"Navigated to Jobs section.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to navigate to Jobs section: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Construct Job Search URL\n",
    "search_url = f\"https://www.linkedin.com/jobs/search/?keywords={designation.replace(' ', '%20')}&location={location.replace(' ', '%20')}\"\n",
    "driver.get(search_url)\n",
    "logging.info(f\"Searching for jobs: {designation} in {location}\")\n",
    "\n",
    "# Extract Job Links\n",
    "job_links = []\n",
    "for page in range(num_pages):\n",
    "    logging.info(f\"Scraping page {page + 1}\")\n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        job_listings = soup.find_all('li', class_=\"old-layout__list-item\")\n",
    "        \n",
    "        for item in job_listings:\n",
    "            link = item.find('a', href=True)\n",
    "            if link:\n",
    "                job_links.append(\"https://www.linkedin.com\" + link['href'])\n",
    "        logging.info(f\"Extracted {len(job_listings)} job links on page {page + 1}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting job links on page {page + 1}: {e}\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f\"//button[@aria-label='Page {page + 2}']\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(2)\n",
    "    except (NoSuchElementException, TimeoutException, ElementClickInterceptedException) as e:\n",
    "        logging.error(f\"Pagination error: {e}\")\n",
    "        break\n",
    "\n",
    "logging.info(f\"Total job links extracted: {len(job_links)}\")\n",
    "\n",
    "# Scrape Job Details\n",
    "job_data = []\n",
    "for link in job_links:\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        job_title = soup.find('div', class_=\"t-24 job-details-jobs-unified-top-card__job-title\")\n",
    "        job_title = job_title.get_text(strip=True) if job_title else \"Title not provided\"\n",
    "\n",
    "        company_name = soup.find('div', class_='job-details-jobs-unified-top-card__company-name')\n",
    "        company_name = company_name.get_text(strip=True) if company_name else \"Company not provided\"\n",
    "\n",
    "        loc_date_div = soup.find('div', class_='job-details-jobs-unified-top-card__primary-description-container')\n",
    "        loc_date_span = loc_date_div.find_all('span', class_='tvm__text tvm__text--low-emphasis') if loc_date_div else []\n",
    "\n",
    "        job_location = \" \".join([span.get_text(strip=True) for span in loc_date_span[:2]]) or \"Location not provided\"\n",
    "        post_date = \" \".join([span.get_text(strip=True) for span in loc_date_span[2:]]) or \"Date not provided\"\n",
    "\n",
    "        work_type_button = soup.find('button', class_='job-details-preferences-and-skills')\n",
    "        work_types = work_type_button.get_text(strip=True) if work_type_button else \"Work type not provided\"\n",
    "\n",
    "        job_details = {\n",
    "            \"Job Title\": job_title,\n",
    "            \"Company\": company_name,\n",
    "            \"Portal Link\": \"https://www.linkedin.com/\",\n",
    "            \"Job Listing Link\": link,\n",
    "            \"Location\": job_location,\n",
    "            \"Date of Posting\": post_date,\n",
    "            \"Work Type\": work_types,\n",
    "        }\n",
    "        job_data.append(job_details)\n",
    "        logging.info(f\"Extracted job: {job_title} at {company_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping job details from {link}: {e}\")\n",
    "\n",
    "# Save Data to JSON\n",
    "output_dir = os.path.join(os.getcwd(), f\"{designation}_{location}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for job in job_data:\n",
    "    try:\n",
    "        file_name = f\"{job['Company'].replace(' ', '_')}_{job['Location'].replace(' ', '_')}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(job, file, indent=4)\n",
    "        logging.info(f\"Saved job details for: {job['Job Title']} at {job['Company']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving job data: {e}\")\n",
    "\n",
    "logging.info(\"All job details saved successfully!\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ffb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
